# LOBâ€‘Bench

*Benchmarking generative models for **Limit Order Book** data (LOBSTER format)*

---

## Table of Contents
1. [Why LOBâ€‘Bench?](#why)
2. [Installation](#installation)
3. [QuickÂ Start](#quickstart)
4. [Benchmarking Workflow](#workflow)
    * [DataÂ Loader](#dataloader)
    * [ScoreÂ Functions](#scores)
    * [DistanceÂ Metrics](#metrics)
    * [ImpactÂ Response](#impact)
5. [Extending the Library](#extend)
6. [Citing](#citing)
7. [Resources](#resources)

---

## <a id="why"></a>Â Why LOBâ€‘Bench?
*   Quantitatively compares **real** and **generated** LOB sequences.
*   Provides readyâ€‘made 1â€‘D score functions and distance metrics (L1, Wassersteinâ€‘1).
*   Measures *conditional* distributions and *impact response* curves to reveal model drift.
*   Fully openâ€‘source and easy to extend with custom scores or metrics.

---

## <a id="installation"></a>Â Installation
```bash
pip install lob_bench
```
Requires PythonÂ â‰¥â€¯3.9 and the usual scientificâ€‘Python stack (`numpy`, `pandas`, `scipy`, `matplotlib`).

---

## <a id="quickstart"></a>Â QuickÂ Start
```python
from lob_bench import data_loading, scoring, impact

# 1Â Â Load data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
loader = data_loading.Simple_Loader(
    cond_path    = "./data/seed_sequences",   # optional
    generated_path = "./data/generated_sequences",
    real_path      = "./data/real_sequences"
)

# 2Â Â Benchmark distributions â”€
scores = {
    "Spread"          : {"fn": scoring.spread},
    "Interarrival"    : {"fn": scoring.interarrival, "Discrete": False},
    "Imbalance"       : {"fn": scoring.imbalance},
    # â€¦ add or replace as you like
}

metrics = {
    "L1"         : scoring.l1_distance,
    "Wasserstein": scoring.wasserstein_distance,
}

results = scoring.run_benchmark(loader, scores=scores, metrics=metrics)

# results is a tuple:
#Â AÂ  distances + bootstrap CIs
#Â BÂ  raw score values + bin indices
#Â CÂ  plotting helpers

# 3Â Â Impact response â”€â”€â”€â”€â”€â”€â”€â”€â”€
impact_curves, impact_score = impact.impact_compare(loader)
```
See `tutorial.ipynb` for a complete walkâ€‘through.

---

## <a id="workflow"></a>Â Benchmarking Workflow
### <a id="dataloader"></a>Â 1. DataÂ Loader
`Simple_Loader` expects three folders, each containing the **same number** of CSV files in LOBSTER format:
* **conditioning sequences** (optional)
* **generated sequences** (model output)
* **real sequences** (ground truth)

### <a id="scores"></a>Â 2. ScoreÂ Functions
Builtâ€‘in (all 1â€‘D):
| Category | Name | Description |
|----------|------|-------------|
| Price | `Spread` | Best askÂ âˆ’Â best bid |
| TimeÂ &Â Flow | `Interarrival` | Time between successive orders (ms) |
| Volume | `AskVolume` / `BidVolume` | Volume on first *n* levels (defaultÂ 10) |
| Imbalance | `OrderbookImbalance` | Volume difference between best bid/ask |
| Depth | `LimitDepth`, `CancelDepth` | Distance of orders from midâ€‘price |
| Level | `LimitLevel`, `CancelLevel` | Price level index (1â€¦n) |
| Durations | `TimeToCancel` | SubmitÂ â†’Â first cancel / modify |

Conditional variants are easyâ€”provide nested dicts with `eval` and `cond` keys.

### <a id="metrics"></a>Â 3. DistanceÂ Metrics
Two metrics ship by default:
* **L1 (Total Variation)** â€“ bounded inÂ [0,â€¯1].
* **Wassersteinâ€‘1 (Earth Mover)** â€“ meanâ€‘variance normalised.

Add your own by passing a callable with signature `(real_values, gen_values) -> float`.

### <a id="impact"></a>Â 4. ImpactÂ Response
`impact_compare(loader)` computes six response curves (MOâ‚€/â‚, LOâ‚€/â‚, CAâ‚€/â‚) followingÂ [EislerÂ *etâ€¯al.*Â 2012]. The mean absolute difference across lags gives a single impact score.

---

## <a id="extend"></a>Â Extending the Library
1. **Custom score** â€“ create a function `my_score(messages, books) -> ndarray` and add it to the `scores` dict.
2. **Custom metric** â€“ add a function to the `metrics` dict.
3. **New loader** â€“ subclass `BaseLoader` for bespoke formats.
Pull requests are welcome!

---

## <a id="citing"></a>Â Citing
If you use LOBâ€‘Bench in academic work, please cite:
```bibtex
@misc{nagy2025lobbenchbenchmarkinggenerativeai,
  title   = {LOB-Bench: Benchmarking Generative AI for Finance -- an Application to Limit Order Book Data},
  author  = {Peer Nagy and Sascha Frey and Kang Li and Bidipta Sarkar and Svitlana Vyetrenko and Stefan Zohren and Ani Calinescu and Jakob Foerster},
  year    = {2025},
  eprint  = {2502.09172},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url     = {https://arxiv.org/abs/2502.09172}
}
```

---

## <a id="resources"></a>Â Resources
* ğŸ“˜Â **Paper:** <https://arxiv.org/abs/2502.09172>
* ğŸ”—Â **Project site:** <https://lobbench.github.io/>
* ğŸ™Â **Source code:** <https://github.com/peernagy/lob_bench>

---

Licensed under **MIT**.
