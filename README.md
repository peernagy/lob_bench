# LOBâ€‘Bench

*Benchmarking generative models for **Limitâ€¯Orderâ€¯Book** data*

---

## TableÂ ofÂ Contents
1. [Why LOBâ€‘Bench?](#why)
2. [Library Architecture](#arch)
3. [Installation](#installation)
4. [QuickÂ Start](#quickstart)
5. [Benchmarking Workflow](#workflow)
6. [Extending](#extend)
7. [Citing](#citing)
8. [Resources](#resources)

---

## <a id="why"></a> WhyÂ LOBâ€‘Bench?
*   Quantitatively compares **real** and **generated** LOB sequences.
*   Readyâ€‘made score functions, distance metrics, and impactâ€‘response analysis.
*   Works outâ€‘ofâ€‘theâ€‘box with LOBSTER CSVs, yet fully extensible.

---

## <a id="arch"></a> LibraryÂ Architecture

### I.Â Distributional Evaluation
For every sequence the library computes **1â€‘D scores** and measures the distance between
real and generated distributions.

*Implemented distance metrics*
* **L1 / Totalâ€¯Variation**
* **Wassersteinâ€‘1**

*Implemented score functions*
| # | Score | Description |
|---|-------|-------------|
| i. | **Spread** | Best ask âˆ’ best bid price |
| ii. | **InterarrivalÂ time** | Time between two successive orders (ms) |
| iii. | **OrderbookÂ imbalance** | Volume difference at best bid/ask |
| iv. | **TimeÂ toÂ cancel** | Order submission â†’ first cancel/modify |
| v./vi. | **Ask/BidÂ volumeÂ (nÂ levels)** | Volume on each side, first *n* levels (defaultÂ 10) |
| vii./viii. | **Limitâ€‘order depths** | Distance of new limit orders from midâ€‘price |
| ix./x. | **Limitâ€‘order levels** | Price level index (1â€¦*n*) of new limits |
| xi./xii. | **Cancelâ€‘order depths** | Distance of cancellations from midâ€‘price |
| xiii./xiv. | **Cancelâ€‘order levels** | Level index (1â€¦*n*) of cancellations |

*Conditional score functions*
| # | Conditional metric |
|---|-------------------|
| i. | Askâ€‘volume (levelÂ 1) **conditional on spread** |
| ii. | Spread **conditional on hourâ€‘ofâ€‘day** |
| iii. | Spread **conditional on volatility** (std.â€¯dev. of returns) |

### II.Â Impactâ€‘Response Evaluation
The priceâ€‘response function is calculated for six event types:
| Code | Event | Midâ€‘price change? |
|------|-------|-------------------|
| `MO_0` | MarketÂ order | âœ— |
| `MO_1` | MarketÂ order | âœ“ |
| `LO_0` | LimitÂ order  | âœ— |
| `LO_1` | LimitÂ order  | âœ“ |
| `CA_0` | Cancel       | âœ— |
| `CA_1` | Cancel       | âœ“ |

For each event type the library plots the response curve and reports the mean
absolute deviation between real and generated curves.

---

## <a id="installation"></a> Installation
```bash
pip install lob_bench
```
Requires PythonÂ â‰¥Â 3.9 plus `numpy`, `pandas`, `scipy`, `matplotlib`.

---

## <a id="quickstart"></a> QuickÂ Start
```python
from lob_bench import data_loading, scoring, impact

# 1Â Â Load data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
loader = data_loading.Simple_Loader(
    cond_path     = "./data/seed_sequences",   # optional
    generated_path = "./data/generated_sequences",
    real_path      = "./data/real_sequences"
)
# Each folder must contain the **same number** of LOBSTERâ€‘format CSVs.

# 2Â Â Define scores & metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
score_cfg = {
    "Spread"       : {"fn": scoring.spread},
    "Interarrival" : {"fn": scoring.interarrival, "Discrete": False},
    "Imbalance"    : {"fn": scoring.imbalance},
    # Conditional example
    "AskVol|Spread": {
        "eval": {"fn": scoring.ask_volume_lvl1},
        "cond": {"fn": scoring.spread}
    },
}

metric_cfg = {
    "L1"         : scoring.l1_distance,
    "Wasserstein": scoring.wasserstein_distance,
}

# 3Â Â Run benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
results = scoring.run_benchmark(loader, score_cfg, metric_cfg)
# returns (distances, raw_scores, plotting_helpers)

# 4Â Â Impact response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
impact_curves, impact_score = impact.impact_compare(loader)
```
See `tutorial.ipynb` for a full walkâ€‘through.

---

## <a id="workflow"></a> BenchmarkingÂ Workflow
1. **Prepare data** â€“ three equalâ€‘length CSV folders (conditioning, generated, real).
2. **Define score & metric dicts** â€“ or import defaults from `lob_bench.defaults`.
3. **Call `run_benchmark`** â€“ get distances, bootstrap CIs, and plotting helpers.
4. **Call `impact_compare`** â€“ analyse six eventâ€‘type response curves.

---

## <a id="extend"></a> Extending
* **Custom score** â€“ implement `my_score(messages, books) -> ndarray` and add to the score dict.
* **Custom metric** â€“ function `(real_vals, gen_vals) -> float`, add to metric dict.
* **Custom loader** â€“ subclass `BaseLoader` for alternative storage formats.

Pull requests are welcomeÂ ğŸ‰.

---

## <a id="citing"></a> Citing
```bibtex
@misc{nagy2025lobbenchbenchmarkinggenerativeai,
  title   = {LOB-Bench: Benchmarking Generative AI for Finance -- an Application to Limit Order Book Data},
  author  = {Peer Nagy and Sascha Frey and Kang Li and Bidipta Sarkar and Svitlana Vyetrenko and Stefan Zohren and Ani Calinescu and Jakob Foerster},
  year    = {2025},
  eprint  = {2502.09172},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url     = {https://arxiv.org/abs/2502.09172}
}
```

---

## <a id="resources"></a> Resources
* ğŸ“„ **Paper:** <https://arxiv.org/abs/2502.09172>
* ğŸŒ **Project site:** <https://lobbench.github.io/>
* ğŸ™ **Source code:** <https://github.com/peernagy/lob_bench>

---

Licensed under **MIT**.
